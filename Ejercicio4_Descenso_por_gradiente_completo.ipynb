{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUNKiwyTRH8DIAAhYCOZ/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarinaRmzG/Neuronal-Networks/blob/main/Ejercicio4_Descenso_por_gradiente_completo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejercicio 4: Descenso por gradiente completo\n",
        "En este notebook implementaremos el algoritmo completo de descenso por gradiente. Para validar que funciona al final lo probaremos en el entrenamiento de una red neuronal simple. Usaremos como conjunto de datos que esta incluído en el archivo data.csv."
      ],
      "metadata": {
        "id": "ArjAp-Bjya6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr3Q-4ftyV6Z",
        "outputId": "9f510a69-c2fd-4b21-c973-652b9abf596e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas numpy y pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Leer un archivo CSV llamado 'data.csv' ubicado en la ruta '/content/drive/MyDrive/Ejercicios IA/'\n",
        "# y cargarlo en un DataFrame llamado 'admissions'\n",
        "admissions = pd.read_csv('/content/drive/MyDrive/Ejercicios IA/data.csv')\n",
        "\n",
        "# Crear variables ficticias para la columna 'rank' y agregarlas al DataFrame 'data'\n",
        "# El resultado se almacena en un nuevo DataFrame llamado 'data'\n",
        "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
        "\n",
        "# Eliminar la columna original 'rank' del DataFrame 'data'\n",
        "data = data.drop('rank', axis=1)\n",
        "\n",
        "# Estandarizar las características 'gre' y 'gpa' en el DataFrame 'data'\n",
        "# Calcula la media y la desviación estándar de cada característica y estandariza los valores\n",
        "for field in ['gre', 'gpa']:\n",
        "    mean, std = data[field].mean(), data[field].std()\n",
        "    data.loc[:, field] = (data[field] - mean) / std\n",
        "\n",
        "# Fijar una semilla para la generación de números aleatorios con fines de reproducibilidad\n",
        "np.random.seed(42)\n",
        "\n",
        "# Seleccionar aleatoriamente el 90% de las filas del DataFrame 'data' para el conjunto de entrenamiento\n",
        "sample = np.random.choice(data.index, size=int(len(data) * 0.9), replace=False)\n",
        "\n",
        "# Dividir el DataFrame 'data' en dos: 'data' (90%) y 'test_data' (10%)\n",
        "data, test_data = data.iloc[sample], data.drop(sample)\n",
        "\n",
        "# Separar las características y los objetivos del conjunto de entrenamiento\n",
        "features, targets = data.drop('admit', axis=1), data['admit']\n",
        "\n",
        "# Separar las características y los objetivos del conjunto de prueba\n",
        "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n"
      ],
      "metadata": {
        "id": "bFnr0kuZyxQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importamos los paquete necesarios\n",
        "import numpy as np\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "\n",
        "# En este ejercicio por propósitos de analizar las salidas utilizaremos la misma semilla para los números aleatorios.\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "ybOiFcWC0Xsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos algunas funciones necesarias\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoide\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "yeVQJHDL3q2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialización de los pesos\n",
        "En un principio no queremos tener todos los pesos en cero porque esto generaría en la salida una predicción nula. Por lo tanto, asignaremos los pesos iniciales de forma aleatoria y cercanos a cero. Otra recomendación es escalar los valores aleatorios es dependencia del número de entradas del nodo (n).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JhZ5hfRy3tch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights.\n",
        "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)"
      ],
      "metadata": {
        "id": "HcLQ2lo23vAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probemos la precisión de la red antes de entrenarla\n",
        "tes_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = tes_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n",
        "\n",
        "# La precisión debe ser mala seguramente."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eiyidwuk3xWI",
        "outputId": "c2206b2c-816e-4ed6-8c96-7a4737df7530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction accuracy: 0.475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hiperparámetros de la red\n",
        "Los hiperpámetros de la red indican el números de veces que itera el método (épocas-epochs), la taza de aprendizaje (learning rate)."
      ],
      "metadata": {
        "id": "x8XkwOiE320o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# número de épocas\n",
        "epochs = 2\n",
        "# tasa de aprendizaje\n",
        "learnrate = 0.5"
      ],
      "metadata": {
        "id": "UEIwiCtE32JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descenso por gradiente completo.\n",
        "El algoritmo de descenso por gradiente de forma iterativa cambia el valor de los pesos de tal forma que se disminuya el error.\n",
        "\n",
        "\n",
        "En la siguiete celda encontrarás la plantilla del algoritmo. Tu misión, si decides aceptarla, es completar el código faltante para que funcione el entrenamiento."
      ],
      "metadata": {
        "id": "FjOhu-rc37Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la tasa de aprendizaje\n",
        "learning_rate = 0.01  # Ajusta este valor según sea necesario\n",
        "\n",
        "for e in range(epochs):\n",
        "    delta_w = np.zeros(weights.shape)\n",
        "\n",
        "    for x, y in zip(features.values, targets):\n",
        "        # Calcula la predicción de la red (logit)\n",
        "        output = sigmoid(np.dot(x, weights))\n",
        "\n",
        "        # Calcula el error\n",
        "        error = y - output\n",
        "\n",
        "        # Calcula el incremento\n",
        "        delta_w += learning_rate * error * output * (1 - output) * x\n",
        "\n",
        "    # Actualiza los pesos\n",
        "    weights += delta_w\n",
        "\n",
        "    # Calcula el error en el conjunto de datos de entrenamiento\n",
        "    if e % (epochs / 10) == 0:\n",
        "        out = sigmoid(np.dot(features, weights))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "        if last_loss is not None and last_loss < loss:\n",
        "            print(\"Train loss:\", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss:\", loss)\n",
        "        last_loss = loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iULLpm63-tQ",
        "outputId": "34512b90-99ec-474e-a9d8-376f38520752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.2544242407118721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos la exactitud de la red\n",
        "Recordemos que"
      ],
      "metadata": {
        "id": "9WIzGX9b4qHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo de la precisión\n",
        "\n",
        "tes_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = tes_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1CGkGsw4rUo",
        "outputId": "70d90c48-bfea-4f13-8eeb-8e46322ef5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction accuracy: 0.500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tal vez obtuvimos la mejor exactidud, pero ¿qué pasará si incrementamos las épocas? O ¿qué es lo que pasará si cambiamos la tasa de aprendizaje?"
      ],
      "metadata": {
        "id": "yVDZCbzi4wwp"
      }
    }
  ]
}