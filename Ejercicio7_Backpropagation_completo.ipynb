{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEjt0skj3K8LIllrpIUqL6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarinaRmzG/Neuronal-Networks/blob/main/Ejercicio7_Backpropagation_completo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamiento con retropropagación\n",
        "En este ejercicio implementaremos el algoritmo de retropropagación dentro del descenso por gradiente para actualizar todos los pesos de la red durante varias épocas. Para entrenar la red usaremos el conjunto de datos de calificaciones que vimos previamente."
      ],
      "metadata": {
        "id": "okiqNATBYYnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S7Ef3adb0SZ",
        "outputId": "241fdb55-a618-456f-e9b7-a44029b62747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contenido del archivo \"data_prep.py\""
      ],
      "metadata": {
        "id": "ETr9EJjTcXbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Leer un archivo CSV llamado 'data.csv' ubicado en la ruta '/content/drive/MyDrive/Ejercicios IA/'\n",
        "# y cargarlo en un DataFrame llamado 'admissions'\n",
        "admissions = pd.read_csv('/content/drive/MyDrive/Ejercicios IA/data.csv')\n",
        "\n",
        "# Crear variables ficticias para la columna 'rank' y agregarlas al DataFrame 'data'\n",
        "# El resultado se almacena en un nuevo DataFrame llamado 'data'\n",
        "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
        "\n",
        "# Eliminar la columna original 'rank' del DataFrame 'data'\n",
        "data = data.drop('rank', axis=1)\n",
        "\n",
        "# Estandarizar las características 'gre' y 'gpa' en el DataFrame 'data'\n",
        "# Calcula la media y la desviación estándar de cada característica y estandariza los valores\n",
        "for field in ['gre', 'gpa']:\n",
        "    mean, std = data[field].mean(), data[field].std()\n",
        "    data.loc[:, field] = (data[field] - mean) / std\n",
        "\n",
        "# Fijar una semilla para la generación de números aleatorios con fines de reproducibilidad\n",
        "np.random.seed(42)\n",
        "\n",
        "# Seleccionar aleatoriamente el 90% de las filas del DataFrame 'data' para el conjunto de entrenamiento\n",
        "sample = np.random.choice(data.index, size=int(len(data) * 0.9), replace=False)\n",
        "\n",
        "# Dividir el DataFrame 'data' en dos: 'data' (90%) y 'test_data' (10%)\n",
        "data, test_data = data.iloc[sample], data.drop(sample)\n",
        "\n",
        "# Separar las características y los objetivos del conjunto de entrenamiento\n",
        "features, targets = data.drop('admit', axis=1), data['admit']\n",
        "\n",
        "# Separar las características y los objetivos del conjunto de prueba\n",
        "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
      ],
      "metadata": {
        "id": "xo04AxDlcKm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGWnYddoYPie"
      },
      "outputs": [],
      "source": [
        "##importamos paquetes y datos\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer una semilla aleatoria para la reproducibilidad de los resultados.\n",
        "np.random.seed(21)"
      ],
      "metadata": {
        "id": "DIMM5L-IYgyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones necesarias\n",
        "#Calcula la derivada de la función sigmoide en un punto 'x'.\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "#Calcula la función sigmoide en un punto 'x'.\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "8k8rcXe4aXTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparámetros\n",
        "n_hidden = 2  # Número de unidades ocultas en la capa oculta de la red.\n",
        "epochs = 900  # Número de épocas o iteraciones de entrenamiento.\n",
        "learnrate = 0.005  # Tasa de aprendizaje, que controla el tamaño de los ajustes de los pesos en cada iteración.\n",
        "\n",
        "# Obtención del número de entradas (features) y el número de ejemplos (n_records) en los datos de entrada.\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None  # Variable para almacenar la pérdida en la última iteración.\n",
        "\n",
        "\n",
        "# Creamos las matrices de los pesos.\n",
        "\"\"\"\n",
        "La matriz de pesos que conecta las entradas a las unidades ocultas se inicializa con valores aleatorios que siguen una distribución normal.\n",
        "La escala de la inicialización se ajusta en función del número de características para evitar gradientes muy grandes o muy pequeños al inicio\n",
        "del entrenamiento.\n",
        "\"\"\"\n",
        "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                        size=(n_features, n_hidden))\n",
        "\"\"\"\n",
        "La matriz de pesos que conecta las unidades ocultas a la capa de salida también se inicializa con valores aleatorios que siguen una distribución normal.\n",
        "La escala de la inicialización se ajusta en función del número de características.\n",
        "\"\"\"\n",
        "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                         size=n_hidden)"
      ],
      "metadata": {
        "id": "ivsscmyxY0jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamiento"
      ],
      "metadata": {
        "id": "rk9K5jleZdAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteramos a lo largo de un número específico de épocas (epochs) para entrenar la red neuronal.\n",
        "for e in range(epochs):\n",
        "    # Inicializamos los acumuladores de cambios en los pesos para cada época.\n",
        "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
        "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
        "    # Iteramos a través de los datos de entrada y los objetivos en lotes.\n",
        "    for x, y in zip(features.values, targets):\n",
        "        ## Forward pass (Paso hacia adelante) ##\n",
        "        # Calcular la entrada y la salida de la capa oculta y la salida de la red.\n",
        "        hidden_input = np.dot(x, weights_input_hidden)\n",
        "        hidden_output = sigmoid(hidden_input)\n",
        "        output = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
        "\n",
        "        ## Backward pass (Paso hacia atrás) ##\n",
        "        # Calcular el error como la diferencia entre la salida real y la predicción de la red.\n",
        "        error = y - output\n",
        "\n",
        "        # Calcular el gradiente de error en la unidad de salida.\n",
        "        output_error = error * sigmoid_prime(output)\n",
        "\n",
        "        # Propagar los errores hacia atrás hasta la capa oculta.\n",
        "        hidden_error = np.dot(output_error, weights_hidden_output) * sigmoid_prime(hidden_input)\n",
        "\n",
        "        # Acumular los cambios en los pesos para su posterior ajuste.\n",
        "        del_w_hidden_output += output_error * hidden_output\n",
        "        del_w_input_hidden += hidden_error * x[:, None]\n",
        "\n",
        "\n",
        "    # Actualizar los pesos en cada época.\n",
        "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
        "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
        "        out = sigmoid(np.dot(hidden_output,\n",
        "                             weights_hidden_output))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
        "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
        "predictions = out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3S-nwcWZfs9",
        "outputId": "b225dab2-578a-419b-c3a1-4d85b9b642cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss:  0.2763009930938133\n",
            "Train loss:  0.27495932757624214\n",
            "Train loss:  0.27364960588577947\n",
            "Train loss:  0.27237115882457463\n",
            "Train loss:  0.2711233242691679\n",
            "Train loss:  0.2699054475787942\n",
            "Train loss:  0.26871688196443266\n",
            "Train loss:  0.26755698882064394\n",
            "Train loss:  0.2664251380221779\n",
            "Train loss:  0.2653207081872718\n",
            "Prediction accuracy: 0.450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusión\n",
        "A medida que se ejecuta, se busca minimizar la pérdida en el conjunto de entrenamiento y evaluar la precisión en un conjunto de prueba. Los hiperparámetros y la inicialización de pesos son esenciales en el proceso de entrenamiento y pueden influir en el rendimiento de la red."
      ],
      "metadata": {
        "id": "Bg9Nzd1ndEM-"
      }
    }
  ]
}